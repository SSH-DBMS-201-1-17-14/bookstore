下面实现给一个查询字符串和需要查询的内容（title之类的），进行分词，得到分词列表terms

然后从表中读取关键词对应的id，得到这些id的重要程度打分（含terms中词项的个数/id对应字符串（例如：标题）本身分词长度）并呈现相应页码。（分页）



1.对包含用户输入分词的id，计算 出现分词次数/它原本的分词数

```python
# 用户输入一个string，得到分词列表terms
def split_user_input(string): 
    new_string = re.sub(r'[^\w\s]', '',string) #只保留中文和英文字符，去掉符号
    terms = jieba.lcut(new_string)  #分词 得到分词列表
    
    #去除为空的
    while ' ' in terms:
           terms.remove(' ')
    while '\n' in terms:
           terms.remove('\n')
        
    return terms
```



2.id : 含用户输入字符串（如‘标题’）分词个数   得到{'1000134': 2, '1009273': 2, '1008732': 1}

```python
# 使用倒排索引表，找到包含terms分词列表的id，再得到它包含的关键词个数: {'1000134': 2, '1009273': 2, '1008732': 1}
# 注，出现多次，则重复添加。 
# 如用户输入“三毛的三毛”，由 split_user_input函数得到 terms = ['三毛','的','三毛'] 
#由find_ids得到 {'1000134': 2, '1009273': 2, '1008732': 1}
def find_ids(terms,index_dic):#['三毛','的','三毛'] 
    term_in_id = {}
    for term in terms:
        if term in index_dic.keys():
            value_ids = index_dic[term]
            for id in value_ids:
                if id in term_in_id.keys():
                    term_in_id[id] = term_in_id[id] + 1
                else:
                    term_in_id[id] = 1
    return term_in_id
```





```python
# 得到重要程度打分 呈现第pagek页（注意函数名加了_pagek）
from math import ceil
def sort_id_importance_pagek(term_in_id,freqdic,pagek):
    importance_score = {}
    #对id
    for id_num in term_in_id.keys():
        word_num =  freqdic[id_num]  #得到id对应字符串本身分词个数
        score = term_in_id[id_num]/word_num*1.0   #得分
        importance_score[id_num ] =score 
        
    #得到所有排序
    importance_score = sorted(importance_score.items(), key=lambda d:d[1], reverse=True) #由分数的从高到低进行排序 
    
    #因为最后一页不一定能显10个，判断一下能分几页
    id_n = len(importance_score)
    pages_num = ceil(id_n/10*1.0)  #共有pages_num
    
    #输入页码超过最大页数,输出空列表
    if pagek>pages_num:
        return []
    
    #输入页码小于等于最大页数
    #分页 第k页：下标（从0开始）  ： 10*(k-1) 到 k*10-1
    start =  10*(pagek-1)
    end = pagek*10-1
    # 如果是最后一页,不一定能输10个，只能输出到最后一个
    if pagek == pages_num:
        end = id_n - 1 
    
    id_l = []  #最后输出的id
    for i in range(start,end+1):
        id_l.append(importance_score[i][0])
    return id_l 
```









================不用管：之前的，未分页的重要度==============================

3 对包含用户输入分词的id，计算 出现分词次数/它原本的分词数

```python
# 得到重要程度打分

'''
输入：{id:所含用户输入关键词个数}和{id:本身分词个数}
得到：一个列表，[(id，重要程度打分)] ，每个元素为一个元组
如：
[('1000134', 0.6666666666666666),
 ('1009273', 0.6666666666666666),
 ('1008732', 0.3333333333333333)]
'''

def sort_id_importance(term_in_id,freqdic):
    importance_score = {}
    #对id
    for id_num in term_in_id.keys():
        word_num =  freqdic[id_num]  #得到id对应字符串本身分词个数
        score = term_in_id[id_num]/word_num*1.0   #得分
        importance_score[id_num ] =score  
    importance_score = sorted(importance_score.items(), key=lambda d:d[1], reverse=True) #由分数的从高到低进行排序
    return importance_score 
```









后面的不用管



```python
# 得到重要程度打分
# 输入：
#term_in_id：{'三毛': ['1000134', '1009273', '1000134', '1009273'], '书': ['1008732']}
# freqdic：id对应字符串（比如标题）的分词数字典 {'1000067': 2,'1000134': 3,……}
def sort_id_importance(term_in_id,freqdic):
    reddic =  mapred(term_in_id)  #此函数见下方
    importance_score = {}
    for id_num in reddic:
        word_num =  freqdic[id_num]
        score = reddic[id_num]/word_num*1.0
        importance_score[id_num ] =score 

    return importance_score 


# 这是上面用到的分函数，不单独调用
# 得到id所包含的用户输入分词个数 返回一个列表
# 例如 输入：{'三毛': ['1000134', '1009273', '1000134', '1009273'], '书': ['1008732']}
# 得到 {'1000134': 2, '1009273': 2, '1008732': 1}
def mapred(termsdic):
    idx = list(termsdic.keys())
    
    term_id_list = []  #map
    for x in idx:
        term_list = termsdic[x]
        for term in term_list:
            term_id_list.append([term,x])
            
    reddic = {} #reduce
    for x in term_id_list:
        #x[0]:词          x[1]:出现的文档id
        if x[0] not in reddic.keys():
            reddic[x[0]] = 1
        else:
            reddic[x[0]] = reddic[x[0]] +1
            
    return reddic 
```

